{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "N-Gram.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPrddogPrzVHfiXu5igmx+x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aguilarafa2010/PLN/blob/main/N_Gram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvwQMoXMZpdc"
      },
      "source": [
        "# Método que quebra as string em N - Gramas\n",
        "\n",
        "def nGram(palavra, n):\n",
        "\n",
        "  digramas = []\n",
        "  count = 0\n",
        "\n",
        "  if len(palavra) > n:\n",
        "    for x in palavra:\n",
        "      if count + n <= len(palavra):\n",
        "        y = palavra[count: count + n]\n",
        "        digramas.append(y)\n",
        "        count = count + 1\n",
        "  return digramas"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16DMMU2BeznY"
      },
      "source": [
        "# Encontra os elementos únicos dos digramas\n",
        "\n",
        "def unicos(x):\n",
        "\n",
        "  unic = []\n",
        "\n",
        "  my_dict = {i:x.count(i) for i in x}\n",
        "\n",
        "  for key, value in my_dict.items(): \n",
        "\n",
        "    if value == 1:\n",
        "      unic.append(key)\n",
        "      \n",
        "  return unic"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBIIJ7yWlmKP"
      },
      "source": [
        "# Calcula a similaridade entre as palavras\n",
        "\n",
        "def similaridade(A, B):\n",
        "\n",
        "  nA = len(A)\n",
        "  nB = len(B)\n",
        "\n",
        "  C = []\n",
        "  for i in A:\n",
        "    if i in B:\n",
        "      C.append(i)\n",
        "  nC = len(C)\n",
        "\n",
        "  return 2*nC / (nA + nB)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raSMwarrZi_C"
      },
      "source": [
        "# Dados de entrada\n",
        "\n",
        "N = 2\n",
        "dados = [\"abacate\", \"abacaxi\", \"abobora\", \"abobrinha\", \"ananás\", \"maça\", \"mamão\",\n",
        "\"manga\", 'melancia', \"melão\", \"mexerica\", \"morango\"] \n",
        "palavra = \"abacati\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "w47WMPuzm5sI",
        "outputId": "43722c0c-47eb-4ef3-b089-276bf90a10af"
      },
      "source": [
        "dPalavra = unicos(nGram(palavra, N))\n",
        "\n",
        "for i in dados:\n",
        "\n",
        "  dDado = unicos(nGram(i, N))\n",
        "  simi = similaridade(dPalavra, dDado)\n",
        "\n",
        "  print(i, simi) "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "abacate 0.8333333333333334\n",
            "abacaxi 0.6666666666666666\n",
            "abobora 0.2\n",
            "abobrinha 0.14285714285714285\n",
            "ananás 0.0\n",
            "maça 0.0\n",
            "mamão 0.0\n",
            "manga 0.0\n",
            "melancia 0.0\n",
            "melão 0.0\n",
            "mexerica 0.15384615384615385\n",
            "morango 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}